{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from DataPantograph import DataPantograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set already exists. Loading the specified data ...\n",
      "\n",
      "Size of training set: 34\n",
      "\n",
      "Size of validation set: 3\n",
      "\n",
      "Size of test set: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_data = \"./Both\"\n",
    "DP_parameters = {'path_name':path_data,\n",
    "                 'data_name':'Test',\n",
    "                 'size':(80,80),\n",
    "                 'augmentation':False,\n",
    "                 'train_percentage':0.63,\n",
    "                 'val_percentage':0.07,\n",
    "                 'test_percentage':0.3,\n",
    "                 'load_extension':'.png'}\n",
    "\n",
    "# (self, path_name, data_name,size, data_augmentation=False, train_percentage=0.63, val_percentage=0.07,\n",
    "#                  test_percentage=0.3,\n",
    "#                  load_extension='.jpg',augmentation = False)\n",
    "\n",
    "DP = DataPantograph(**DP_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set already exists. Loading the specified data ...\n",
      "\n",
      "Size of training set: 34\n",
      "\n",
      "Size of validation set: 3\n",
      "\n",
      "Size of test set: 15\n",
      "\n",
      "[<tf.Tensor 'input_1:0' shape=(?, ?, ?, 3) dtype=float32>, <tf.Tensor 'batch_normalization_2/cond/Merge:0' shape=(?, ?, ?, 32) dtype=float32>, <tf.Tensor 'max_pooling2d_1/MaxPool:0' shape=(?, ?, ?, 32) dtype=float32>, <tf.Tensor 'batch_normalization_4/cond/Merge:0' shape=(?, ?, ?, 64) dtype=float32>, <tf.Tensor 'max_pooling2d_2/MaxPool:0' shape=(?, ?, ?, 64) dtype=float32>, <tf.Tensor 'batch_normalization_6/cond/Merge:0' shape=(?, ?, ?, 128) dtype=float32>, <tf.Tensor 'concatenate_1/concat:0' shape=(?, ?, ?, 192) dtype=float32>, <tf.Tensor 'batch_normalization_8/cond/Merge:0' shape=(?, ?, ?, 64) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, ?, ?, 96) dtype=float32>, <tf.Tensor 'batch_normalization_10/cond/Merge:0' shape=(?, ?, ?, 32) dtype=float32>, <tf.Tensor 'conv2d_11/Sigmoid:0' shape=(?, ?, ?, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "from PantographHardExamples import PantographHardExamples\n",
    "from keras.optimizers import Adam\n",
    "from PantographNetwork import jaccard_coef,dice_coef\n",
    "path_data = \"./Both\"\n",
    "PHE_parameters = {'path_name':path_data,\n",
    "                        'size':(80,80),\n",
    "                        'data_name':'Test',\n",
    "                        'channels': 3,\n",
    "                        'classes': 2,\n",
    "                        'num_layers': 3,\n",
    "                        'loss': \"binary_crossentropy\",\n",
    "                        'optimizer': Adam(),\n",
    "                        'data_augmentation':False,\n",
    "                        'train_percentage':0.63,\n",
    "                        'val_percentage':0.07,\n",
    "                        'test_percentage':0.3,\n",
    "                        'data_type':'.tiff',\n",
    "                        'file_type_load':'.png',\n",
    "                        'batch_normalization':True,\n",
    "                        'dropout_type':0,\n",
    "                        'dropout_p':0.5}\n",
    "\n",
    "\n",
    "PHE = PantographHardExamples(**PHE_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the model are: \n",
      "\n",
      "Layers :: 3 \n",
      "\n",
      "Size training batch :: 1 \n",
      "\n",
      "Size validation batch :: 1 \n",
      "\n",
      "Epochs :: 2 \n",
      "\n",
      "Modified data :: 0 \n",
      "\n",
      "Train data used = 471\n",
      "\n",
      "Validation data used = 3\n",
      "\n",
      "Epoch 1/2\n",
      "470/471 [============================>.] - ETA: 0s - loss: 0.2656 - jaccard_coef: 0.1075 - dice_coef: 0.4260"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Anaconda3\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\Anaconda3\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 630, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"C:\\Users\\Luis Pérez\\Documents\\Master\\TUE\\Internship Sioux\\Python\\Final Training\\Pantograph\\DataGeneratorPantograph.py\", line 38, in generate\n",
      "    X, y = self.__data_generation(batch_names)\n",
      "  File \"C:\\Users\\Luis Pérez\\Documents\\Master\\TUE\\Internship Sioux\\Python\\Final Training\\Pantograph\\DataGeneratorPantograph.py\", line 47, in __data_generation\n",
      "    X[i, :, :, :] = load_image(self.path, name)  # Load the data for the image\n",
      "ValueError: could not broadcast input array from shape (256,1600,3) into shape (172,1500,3)\n",
      "\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-11a3b30c8919>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                         \u001b[1;34m'epochs_per_training'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                         'hard_example_epochs':1}\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mPHE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhard_example_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mPHE_train_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Master\\TUE\\Internship Sioux\\Python\\Final Training\\Pantograph\\PantographHardExamples.py\u001b[0m in \u001b[0;36mhard_example_training\u001b[1;34m(self, prefix_log, HE_per_image, batch_size_train, batch_size_val, epochs_per_training, hard_example_epochs)\u001b[0m\n\u001b[0;32m     71\u001b[0m             self.UNETnet.train_model(prefix_log=prefix_log, batch_size_train=batch_size_train,\n\u001b[0;32m     72\u001b[0m                                      \u001b[0mbatch_size_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                                      epochs=epochs_per_training)\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhard_example_iteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhard_example_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhard_example_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Master\\TUE\\Internship Sioux\\Python\\Final Training\\Pantograph\\PantographNetwork.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, prefix_log, batch_size_train, batch_size_val, epochs)\u001b[0m\n\u001b[0;32m    112\u001b[0m                                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                                  \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                                  callbacks=[TB, CP])\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprefix_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparameter_string\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m' .h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprefix_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m' BEST '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparameter_string\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m' .h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2116\u001b[0m                                 \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2118\u001b[1;33m                                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   2119\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2120\u001b[0m                             \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2216\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2217\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2218\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2219\u001b[0m                     raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PHE_train_parameters = {'prefix_log':'Whatever',\n",
    "                        'HE_per_image':1,\n",
    "                        'batch_size_train': 1,\n",
    "                        'batch_size_val':1,\n",
    "                        'epochs_per_training':2,\n",
    "                        'hard_example_epochs':1}\n",
    "PHE.hard_example_training(**PHE_train_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4c1c5f1885ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPantographNetwork\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjaccard_coef\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdice_coef\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPantographNetwork\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPantographNetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m PN_parameters = {'DataPantograph': DP,\n\u001b[0m\u001b[0;32m      5\u001b[0m                             \u001b[1;34m'channels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                             \u001b[1;34m'classes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DP' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from PantographNetwork import jaccard_coef,dice_coef\n",
    "from PantographNetwork import PantographNetwork\n",
    "PN_parameters = {'DataPantograph': DP,\n",
    "                            'channels': 3,\n",
    "                            'classes': 2,\n",
    "                            'initial_features': 32,\n",
    "                            'num_layers': 3,\n",
    "                            'loss': \"binary_crossentropy\",\n",
    "                            'optimizer': Adam(),\n",
    "                            'metrics':[jaccard_coef,dice_coef],\n",
    "                            'data_type':'.tiff',\n",
    "                            'batch_normalization':True,\n",
    "                            'dropout_type':0,\n",
    "                            'dropout_p':1.0}\n",
    "PN = PantographNetwork(**PN_parameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the model are: \n",
      "\n",
      "Layers :: 3 \n",
      "\n",
      "Size training batch :: 10 \n",
      "\n",
      "Size validation batch :: 1 \n",
      "\n",
      "Epochs :: 2 \n",
      "\n",
      "Modified data :: 0 \n",
      "\n",
      "Train data used = 471\n",
      "\n",
      "Validation data used = 3\n",
      "\n",
      "Epoch 1/2\n",
      " 5/47 [==>...........................] - ETA: 2:33 - loss: 0.6371 - jaccard_coef: 0.1437 - dice_coef: 0.2750"
     ]
    }
   ],
   "source": [
    "train_parameters = {'prefix_log':'First_Try',\n",
    "                        'batch_size_train':10,\n",
    "                        'batch_size_val':1,\n",
    "                        'epochs':2}\n",
    "PN.train_model(**train_parameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model,model_name = AM.extract_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'Statistics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-173748c1b4f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mStatistics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAsbestosModelProcess\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'Statistics'"
     ]
    }
   ],
   "source": [
    "from Statistics import AsbestosModelProcess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
